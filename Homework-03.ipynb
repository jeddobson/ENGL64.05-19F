{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Cultural Analytics: Homework #3</h1></center>\n",
    "\n",
    "<center><b>Due</b> 11:59PM 10/13/2019</center>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's begin by loading up some libraries\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tsvdro import tsvdro\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One: Similarity with Close Reading\n",
    "\n",
    "You'll need to locate a sonnet or some other object of similar length (fourteen lines) and word count. Select and \"cut\" this into a buffer to paste below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here are going to load some previously saved data.\n",
    "# These are sonnets from eighteenth century sources. Many of these\n",
    "# are much older than the eighteenth century, however.\n",
    "#\n",
    "\n",
    "fp = open('data/mmlec/labeled_objects_mmlec_sonnets.pkl','rb')\n",
    "mmlec_sonnets = pickle.load(fp)\n",
    "\n",
    "# how many sample sonnets do we have?\n",
    "len(mmlec_sonnets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now cut and past a sample poem or section of text into the space below\n",
    "sample_sonnet = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# tokenize (for later) and also add to our data\n",
    "sample_tokens = word_tokenize(sample_sonnet)\n",
    "mmlec_sonnets.append(sample_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we're dealing with small objects, so let's try the binary vector method\n",
    "\n",
    "# Import CountVectorizer and configure options\n",
    "# to learn more about the various options, see the\n",
    "# documentation for this function:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "vectorizer = text.CountVectorizer(input='content',\n",
    "                                  lowercase='true',\n",
    "                                  binary='true',\n",
    "                                  strip_accents='unicode')\n",
    "\n",
    "# This does the actual vectorization\n",
    "counts = vectorizer.fit_transform(mmlec_sonnets)\n",
    "\n",
    "# get vocabulary for later\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# Return total number of documents and the number of items in the vocabulary\n",
    "dc, vc = counts.shape\n",
    "print(\"document count:\",dc,\"vocabulary count:\",vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate distance between each sonnet\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_dist_matrix = 1 - cosine_similarity(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find similar texts\n",
    "\n",
    "difference_list=dict()\n",
    "sample_text = len(mmlec_sonnets) - 1\n",
    "for i, text in enumerate(mmlec_sonnets): \n",
    "    difference_list[i] = np.round(cosine_dist_matrix[sample_text,i],3)\n",
    "    i += 1\n",
    "    \n",
    "# display sorted distances in vector space from our selected text\n",
    "x = 0\n",
    "for i in sorted(difference_list, key=difference_list.__getitem__):\n",
    "    if x < 100:\n",
    "        print(\"\\n------------------------------------------------------\\n\")\n",
    "        matched_tokens = word_tokenize(mmlec_sonnets[i])\n",
    "        for word in matched_tokens:\n",
    "            if word in sample_tokens and word in vocabulary:\n",
    "                print(\"*{0}*\".format(word),end = ' ')\n",
    "            else:\n",
    "                print(word, end = ' ')\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question #1:</b> What observations can you make about the sonnets returned as most similar (they are returned in order of similarity)? What is being returned? Explain what you take to be the signficance of the returned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two: Plotting Genre\n",
    "\n",
    "The following cells will read in Ted Underwood's metadata for Chapter 1 and then plot/visualize these genres. You will need to select a number of texts to pick from each genre. Replace 'XXX' with a number for the n_documents variable. Execute/Run the remainder of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This lines tell the Python interpreter to load some additional functions.\n",
    "# We need functions to parse Comma Separated Value (CSV) data.\n",
    "import csv\n",
    "\n",
    "# we'll set a variable to hold the number of records read from this CSV file.\n",
    "ln = 0\n",
    "\n",
    "# as above, we'll create a metadata variable (overwritting the previous variable)\n",
    "metadata=list()\n",
    "\n",
    "# Now we open the CSV file and read each line, appending to the metadata list:\n",
    "with open('data/Underwood_ch1/allgenremeta.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    for row in reader:\n",
    "        metadata.append(row)\n",
    "        \n",
    "        # increment our counter\n",
    "        ln += 1\n",
    "\n",
    "# tell us how many entries we've read\n",
    "print(\"read %s lines\" % ln)\n",
    "\n",
    "# Let's remove the header (our \"metadata\") for easier processing:\n",
    "metadata = metadata[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select:\n",
    "#\n",
    "# fifty works of fiction\n",
    "# fifty works of poetry\n",
    "# fifty works of biography\n",
    "\n",
    "# How many texts per category?\n",
    "n_documents = XXX\n",
    "\n",
    "\n",
    "selected_documents=list()\n",
    "selected_metadata=list()\n",
    "\n",
    "counters=dict()\n",
    "counters['bio'] = 0\n",
    "counters['fic'] = 0\n",
    "counters['poe'] = 0\n",
    "\n",
    "def load_and_add(short_name):\n",
    "    fn = 'data/Underwood_ch1/' + short_name + \".dro\"\n",
    "    dro = tsvdro.load(fn)\n",
    "\n",
    "    selected_documents.append(dro['data'])\n",
    "    selected_metadata.append(dro['header'])\n",
    "    \n",
    "for document in metadata:\n",
    "    if document[5] == \"bio\" and counters['bio'] < n_documents:\n",
    "            load_and_add(document[0])\n",
    "            counters['bio'] += 1\n",
    "    if document[5] == \"fic\" and counters['fic'] < n_documents:\n",
    "            load_and_add(document[0])\n",
    "            counters['fic'] += 1\n",
    "    if document[5] == \"poe\" and counters['poe'] < n_documents:\n",
    "            load_and_add(document[0])\n",
    "            counters['poe'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "counts = vectorizer.fit_transform(selected_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return total number of documents and the number of items in the vocabulary\n",
    "dc, vc = counts.shape\n",
    "print(\"document count:\",dc,\"vocabulary count:\",vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = counts.toarray()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_dist_matrix = 1 - cosine_similarity(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(cosine_dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize.\n",
    "# \n",
    "# NOTE: you might need to click on the image to expand it enough to read it.\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "fig = plt.figure(figsize=(35, 20), dpi=75)\n",
    "genres = [x['bibliographic_data']['genre'] for x in selected_metadata]\n",
    "for x, y, genre in zip(xs, ys, genres):\n",
    "    plt.scatter(x, y, c='black',s=10)\n",
    "    plt.text(x, y, genre)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<b>Question #2:</b>: Examine the above genre \"map.\" Can you separate poetry from fiction from biography? What might this mean? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
