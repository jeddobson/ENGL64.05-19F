{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Cultural Analytics: Homework #4</h1></center>\n",
    "\n",
    "<center><b>Due</b> 11:59PM 11/03/2019</center>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "import statistics\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Bing Sentiment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load Bing sentiment lexicons for positive and negative words\n",
    "bing_positive = open('data/lexicons/positive-words.txt').read()\n",
    "bing_negative = open('data/lexicons/negative-words.txt').read()\n",
    "\n",
    "# convert to lists\n",
    "bing_positive = bing_positive.split('\\n')\n",
    "bing_negative = bing_negative.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# give a count of terms\n",
    "print(\"Found {0} negative terms\".format(len(bing_negative)))\n",
    "print(\"Found {0} positive terms\".format(len(bing_positive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many stop words are list?\n",
    "\n",
    "# load Scikit-learn stop words\n",
    "stop_words = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "# check our lists\n",
    "for word in stop_words:\n",
    "    if word in bing_negative:\n",
    "        print(\"negative:\",word)\n",
    "    if word in bing_positive:\n",
    "        print(\"positive:\",word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to assess orientation of sample text\n",
    "#\n",
    "# this function should return neutral, positive, or negative \n",
    "# for the entire supplied text\n",
    "\n",
    "def sentiment_orientation(text):\n",
    "    \n",
    "    # determine if we need to tokenize or not\n",
    "    if type(text) == str:\n",
    "        # tokenize string\n",
    "        text = word_tokenize(text)\n",
    "        \n",
    "    return(orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now evaluate a sample review that you found on the Web\n",
    "review = \"\"\"PASTE REVIEW CONTENTS HERE\n",
    "\"\"\"\n",
    "# this should display either positive, neutral, or negative\n",
    "print(sentiment_orientation(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read and Convert Hawthorne's The Scarlet Letter into 100 Units\n",
    "input_text = \"data/Novel450/EN_1850_Hawthorne,Nathaniel_TheScarletLetter_Novel.txt\"\n",
    "\n",
    "# make into a collection of 100 documents\n",
    "raw_text = open(input_text).read()\n",
    "tokens = word_tokenize(raw_text)\n",
    "segment_length =  int(len(tokens)/100)\n",
    "collection = list()\n",
    "\n",
    "for i in range(100):\n",
    "        segment = tokens[segment_length*i:segment_length*(i+1)]\n",
    "        collection.append(' '.join(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to determine the mean sentiment of supplied text\n",
    "# this should return the mean sentiment using the Bing lexicon\n",
    "#\n",
    "\n",
    "def sentiment_means(text):\n",
    "    \n",
    "    # determine if we need to tokenize or not\n",
    "    if type(text) == str:\n",
    "        # tokenize string\n",
    "        text = word_tokenize(text)\n",
    "    \n",
    "    # HINT:\n",
    "    # we've imported the statistics package, which contains a function called 'mean'\n",
    "    # that will return the mean of a list of values.\n",
    "    # \n",
    "\n",
    "    return(mean_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain mean sentiment value for each segment\n",
    "sentiment_data = list()\n",
    "for segment in collection:\n",
    "    sentiment_data.append(sentiment_means(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to rescale these values from -1 to 1 using the following\n",
    "# function:\n",
    "\n",
    "def rescale(values):\n",
    "    scaled = 2 * (values - min(values))/( max(values) - min(values)) -1\n",
    "    return(scaled)\n",
    "\n",
    "# we can convert our list to a numpy array and then perform a vector operation\n",
    "# on all the data to scale it.\n",
    "data = np.asarray(sentiment_data)\n",
    "scaled = rescale(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now plot your extracted data:\n",
    "# - label x and y axis\n",
    "# - give it a title\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 1:</b> How well does the Bing lexicon work for sentiment scoring? What did you discover in your examination of sentiment in Hawthorne's novel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: labMT Sentiment  \n",
    "\n",
    "This section uses the average scores from the labMT dataset. We'll first examine the dictionary and ask some questions about it before comparing this dictionary to the Bing lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll load the CSV file and put everything into a Python dictionary\n",
    "import csv\n",
    "labMT_sentiment = dict()\n",
    "with open('data/lexicons/word_sentiment_scores.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    for row in reader:\n",
    "        labMT_sentiment[row[0]] = float(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 10222 words in labMT lexicon\n"
     ]
    }
   ],
   "source": [
    "# how many words?\n",
    "print(\"found {0} words in labMT lexicon\".format(len(labMT_sentiment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['laughter',\n",
       " 'happiness',\n",
       " 'love',\n",
       " 'happy',\n",
       " 'laughed',\n",
       " 'laugh',\n",
       " 'laughing',\n",
       " 'laughs',\n",
       " 'excellent',\n",
       " 'joy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the top ten happiest words?\n",
    "sorted(labMT_sentiment, key=labMT_sentiment.__getitem__,reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terrorist',\n",
       " 'suicide',\n",
       " 'rape',\n",
       " 'terrorism',\n",
       " 'murder',\n",
       " 'cancer',\n",
       " 'death',\n",
       " 'died',\n",
       " 'kill',\n",
       " 'killed']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are top ten least happiest words?\n",
    "sorted(labMT_sentiment, key=labMT_sentiment.__getitem__)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"max sentiment\",max(labMT_sentiment.values()))\n",
    "print(\"min sentiment\",min(labMT_sentiment.values()))\n",
    "print(\"median sentiment\",statistics.median(labMT_sentiment.values()))\n",
    "print(\"mean sentiment\",statistics.mean(labMT_sentiment.values()))\n",
    "print(\"mode sentiment\",statistics.mode(labMT_sentiment.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract words scored as the mode\n",
    "mode_score = statistics.mode(labMT_sentiment.values())\n",
    "[x for x in labMT_sentiment if labMT_sentiment[x] == mode_score ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #2:</b> What do you make of these scores and these terms? Do you feel this is an appropriate range of words and scores? For what purposes? What do you make of the \"neutral\" words? What problems or limitations might you identify in using this lexicon for cultural analytics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Scoring Texts with labMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to determine the mean sentiment of supplied text\n",
    "# this should return the mean sentiment using the labMT lexicon\n",
    "#\n",
    "\n",
    "def labMT_sentiment_means(text):\n",
    "    \n",
    "    # determine if we need to tokenize or not\n",
    "    if type(text) == str:\n",
    "        # tokenize string\n",
    "        text = word_tokenize(text)\n",
    "    \n",
    "    # HINT 1:\n",
    "    # we've imported the statistics package, which contains a function called 'mean'\n",
    "    # that will return the mean of a list of values.\n",
    "    # \n",
    "    \n",
    "    # HINT 2:\n",
    "    # You can easily check if a word exists in a dictionary with an 'if' statement\n",
    "    # if word in dictionary:\n",
    "    #     value = dictionary[word]\n",
    "    #\n",
    "    \n",
    "    return(mean_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now select a text from the 'data/Novel450' or 'data/na-slave-narratives/data/texts' directory \n",
    "# and segment into 100 segments and determine the mean sentiment for each segment and plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question 3:</b> Compare the results of the mean scores for both th Bing and labMT sentiment lexicon. What other techniques might you use to calculate sentiment scores with these lexicons? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
