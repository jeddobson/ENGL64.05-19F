{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Cultural Analytics</h1><br>\n",
    "<h2>ENGL64.05 Fall 2019</h2>\n",
    "</center>\n",
    "\n",
    "----\n",
    "\n",
    "# Lab 3\n",
    "## Segments, Text Processing, and Part of Speech Tagging \n",
    "\n",
    " <center><pre>Rev: 10/09/2019</pre></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Red\">Part One: Part of Speech Tagging</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's begin by loading up some important libraries\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# allow for displaying of graphics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's learn about Part of Speech Tagging. \n",
    "# Write a sample sentence here...\n",
    "\n",
    "test_sentence = \"TEXT GOES HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We need to tokenize a sentence in order to tag the words.\n",
    "test_sentence_tokens = word_tokenize(test_sentence)\n",
    "\n",
    "# Now we run the tagger:\n",
    "nltk.pos_tag(test_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the list of tag types appears at the bottom of this notebook\n",
    "\n",
    "# Now let's return to the second cell and write some other kinds of sentences.\n",
    "# Experiment with words that could be nouns or verbs depending on context.\n",
    "# How well does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can use this \"read\" and quantify text not into word vectors but as \n",
    "# an arrangement or pattern of specific parts of speech.\n",
    "\n",
    "# This is a paragraph from F. Scott Fitzgerald's This Side of Paradise (his Princeton novel)\n",
    "this_side_of_paradise=\"\"\"\n",
    "Amory was far from contented. He missed the place he had won at St. Regis', the being known and admired, \n",
    "yet Princeton stimulated him, and there were many things ahead calculated to arouse the Machiavelli latent \n",
    "in him, could he but insert a wedge. The upper-class clubs, concerning which he had pumped a reluctant graduate \n",
    "during the previous summer, excited his curiosity: Ivy, detached and breathlessly aristocratic; Cottage, \n",
    "an impressive m√©lange of brilliant adventurers and well-dressed philanderers; Tiger Inn, broad-shouldered and \n",
    "athletic, vitalized by an honest elaboration of prep-school standards; Cap and Gown, anti-alcoholic, faintly \n",
    "religious and politically powerful; flamboyant Colonial; literary Quadrangle; and the dozen others, varying in \n",
    "age and position.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we'll tokenize, tag it, and display what we've found:\n",
    "\n",
    "# We need to tokenize a sentence in order to tag the words.\n",
    "p_tokens = word_tokenize(this_side_of_paradise)\n",
    "\n",
    "# Now we run the tagger:\n",
    "tagged = nltk.pos_tag(p_tokens)\n",
    "\n",
    "# find the set of found objects:\n",
    "for tag_type in set([x[1] for x in tagged if x[1].isalpha()]):\n",
    "    print(\"{0}: \".format(tag_type),end='')\n",
    "    for x in tagged:\n",
    "        if x[1] == tag_type:\n",
    "            print(x[0],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's convert our tokens for that text into a NLTK Text object.\n",
    "p_text = nltk.Text(p_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What can we do with this type of object?\n",
    "help(p_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_text.concordance(\"he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try another method from the above list.\n",
    "p_text.XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's open the entire text of This Side of Paradise\n",
    "\n",
    "raw_text = open('data/Novel450/EN_1920_Fitzgerald,FScott_ThisSideofParadise_Novel.txt').read()\n",
    "tokens =  word_tokenize(raw_text)\n",
    "nltk_text_object = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's a sample (collocation as a list).\n",
    "# Try some other things\n",
    "nltk_text_object.collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is this showing us?\n",
    "nltk_text_object.dispersion_plot([\"Amory\",\"Rosalind\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Red\">Part Two: Named Entity Recognition</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use the 'Named Entity Chunker' ne_chunk to 'chunk' our tagged \n",
    "# tokens and then apply named entity recongition.\n",
    "#\n",
    "\n",
    "ner_data = ne_chunk(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|NER|Example|\n",
    "|------------|-----------|\n",
    "|ORGANIZATION|Georgia-Pacific Corp., WHO|\n",
    "|PERSON|Eddy Bonte, President Obama|\n",
    "|LOCATION|Murray River, Mount Everest|\n",
    "|DATE|June, 2008-06-29|\n",
    "|TIME|two fifty a m, 1:30 p.m.|\n",
    "|MONEY|175 million Canadian Dollars, GBP 10.40|\n",
    "|PERCENT|twenty pct, 18.75 %|\n",
    "|FACILITY|Washington Monument, Stonehenge|\n",
    "|GPE|South East Asia, Midlothian|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll make a dictonary to store found Named Entities\n",
    "found_objects = dict()\n",
    "\n",
    "# GPE \n",
    "for i in ner_data.subtrees():\n",
    "    if i.label() == 'GPE': \n",
    "            ner_object = i[0][0]\n",
    "            if ner_object in found_objects:\n",
    "                found_objects[ner_object] += 1\n",
    "            else:\n",
    "                found_objects[ner_object] = 1\n",
    "\n",
    "\n",
    "top_objects = sorted(found_objects, key=found_objects.get, reverse=True)\n",
    "for i in top_objects:\n",
    "    print(i,found_objects[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Edit the above and try some other searches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now try another text. Go back to the raw_text line and swap for another.\n",
    "# Here are some possibilities:\n",
    "\n",
    "# data/Novel450/EN_1818_Shelley,Mary_Frankenstein_Novel.txt\n",
    "# data/Novel450/EN_1847_Bronte,Emily_WutheringHeights_Novel.txt\n",
    "# data/Novel450/EN_1884_Twain,Mark_TheAdventuresofHuckleberryFinn_Novel.txt\n",
    "# data/Novel450/EN_1869_Alcott,Louisa_LittleWomen_Novel.txt\n",
    "# data/Novel450/EN_1890_Wilde,Oscar_ThePictureofDorianGray_Novel.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Red\">Part Three: Segmentation</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Many, many tasks require us to compare units of texts. Let's look at\n",
    "# creating segments of text that can be used for comparing the internal\n",
    "# distribution of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the \"raw text\" of Virginia Woolf's Mrs. Dalloway\n",
    "raw_text = open(\"data/Novel450/EN_1925_Woolf,Virginia_Mrs.Dalloway_Novel.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Okay, let's determine how long it is (word count)\n",
    "\n",
    "# the sent_tokenizer is new for us--this attempts to give us individual sentences.\n",
    "sentences = sent_tokenize(raw_text)\n",
    "print(\"found\",len(sentences),\"sentences\")\n",
    "\n",
    "# our old friend, the word_tokenizer\n",
    "words = word_tokenize(raw_text)\n",
    "print(\"found\",len(words),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What if we created segments of 1,000 words?\n",
    "length = 1000\n",
    "i = 0 \n",
    "segment_list = list()\n",
    "segment=list()\n",
    "n_words = len(words)\n",
    "\n",
    "# here is a process that will enable us to deal\n",
    "# the remainder of words for the final, shorter\n",
    "# segment. Maybe we don't want this?\n",
    "for x, word in enumerate(words):\n",
    "    if i < length and x != n_words -1:\n",
    "        segment.append(word)\n",
    "        i += 1\n",
    "    elif i < length and x == n_words -1:\n",
    "        segment_list.append(segment)\n",
    "    else:\n",
    "        segment_list.append(segment)\n",
    "        segment=list()\n",
    "        i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many buckets did we create?\n",
    "print(len(segment_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Want to try again with a different length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part of Speech Tagging\n",
    "\n",
    "# Let's begin with the first bucket\n",
    "pos_data = nltk.pos_tag(segment_list[0])\n",
    "\n",
    "# find all the proper nouns (NNP)\n",
    "found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "print(len(set(found_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display them\n",
    "found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is our percent of proper nouns per bucket?\n",
    "\n",
    "data_to_plot=list()\n",
    "for seg in segment_list:\n",
    "    total_tokens = len(seg)\n",
    "    pos_data = nltk.pos_tag(seg)\n",
    "    found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "    data_to_plot.append((round(len(found_words)/total_tokens * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display as chart\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(data_to_plot))\n",
    "plt.bar(x, data_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What is this? What can this distribution of the percentage of\n",
    "# proper nouns tell us?\n",
    "\n",
    "# What else could we determine by processing and comparing segments of text?\n",
    "# - Places mentioned \n",
    "# - Type/Token Ratio\n",
    "# - Others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "----\n",
    "\n",
    "|Tag|Meaning|\n",
    "|---|-------|\n",
    "|CC|coordinating conjunction|\n",
    "|CD|cardinal digit|\n",
    "|DT|determiner|\n",
    "|EX|existential there|\n",
    "|FW|foreign word|\n",
    "|IN|preposition/subordinating conjunction|\n",
    "|JJ|adjective|\n",
    "|JJR|adjective, comparative|\n",
    "|JJS|adjective, superlative|\n",
    "|LS|list marker|\n",
    "|MD|modal|\n",
    "|NN|noun, singular|\n",
    "|NNS|noun plural|\n",
    "|NNP|proper noun, singular|\n",
    "|NNPS|proper noun, plural|\n",
    "|PDT|predeterminer|\n",
    "|POS|possessive ending|\n",
    "|PRP|personal pronoun|\n",
    "|PRP$|possessive pronoun|\n",
    "|RB|adverb|\n",
    "|RBR|adverb, comparative|\n",
    "|RBS|adverb, superlative|\n",
    "|RP|particle|\n",
    "|TO|to go|\n",
    "|UH|interjection|\n",
    "|VB|verb, base form|\n",
    "|VBD|verb, past tense|\n",
    "|VBG|verb, gerund/present participle|\n",
    "|VBN|verb, past participle|\n",
    "|VBP|verb, sing. present|\n",
    "|VBZ|verb, 3rd person sing. present|\n",
    "|WDT|wh-determiner which|\n",
    "|WP|wh-pronoun who, what|\n",
    "|WP\\$|possessive pronoun|\n",
    "|WRB|wh-abverb where, when|\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
