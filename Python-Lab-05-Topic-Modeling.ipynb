{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Cultural Analytics</h1><br>\n",
    "<h2>ENGL64.05 Fall 2019</h2>\n",
    "</center>\n",
    "\n",
    "----\n",
    "\n",
    "# Lab 5\n",
    "## Topic Modeling\n",
    "\n",
    " <center><pre>Rev: 10/24/2019</pre></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Single Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find, Download, and Upload a plain-text document to vectorize\n",
    "\n",
    "# make this variable's value the name of the file that you've uploaded\n",
    "input_text = \"\"\n",
    "\n",
    "# make into a collection of 100 documents\n",
    "raw_text = open(input_text).read()\n",
    "tokens = word_tokenize(raw_text)\n",
    "segment_length =  int(len(tokens)/100)\n",
    "collection = list()\n",
    "\n",
    "for i in range(100):\n",
    "        segment = tokens[segment_length*i:segment_length*(i+1)]\n",
    "        collection.append(' '.join(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup vectorizer and process text\n",
    "vec = CountVectorizer(input='content',\n",
    "                      stop_words='english',\n",
    "                      min_df=2,\n",
    "                      lowercase=True)\n",
    "\n",
    "counts = vec.fit_transform(collection)\n",
    "dc, vc = counts.shape\n",
    "print(\"read {0} documents with {1} vocabulary\".format(dc,vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the LDA Model\n",
    "#\n",
    "# n_components = number of topics to extract (if topics are too similar, extract more)\n",
    "# \n",
    "simple_lda_model = LatentDirichletAllocation(n_components=2,\n",
    "                                             max_iter=5,\n",
    "                                             learning_method='batch',\n",
    "                                             random_state=1)    \n",
    "# get fitted data and transformed matrix\n",
    "simple_lda_data = simple_lda_model.fit(counts)\n",
    "\n",
    "# extract the features to a simple list\n",
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "# how many words do we want to extract for each topic?\n",
    "n_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now produce topics\n",
    "for topic_idx, topic in enumerate(simple_lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "        print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: LDA on Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load some data from one of our collections (in the \"data\" directory)\n",
    "# create a variable that is a list of file names\n",
    "# HINT: use the glob() function\n",
    "\n",
    "novel_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# come back here later....\n",
    "# custom stopword list...\n",
    "# this loads the existing stopwords from Scikit-Learn\n",
    "stopwords = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "# now we need to add values to this list.\n",
    "# how can you update an existing list?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop_words can be 'english' or a variable name that is a list\n",
    "vectorizer = CountVectorizer(input='filename',\n",
    "                            stop_words='english',\n",
    "                            min_df=.5, # words must be present in 1/2 of the documents\n",
    "                            lowercase=True) # lowercase is a Boolean True/False\n",
    "\n",
    "counts = vectorizer.fit_transform(novel_data)\n",
    "dc, vc = counts.shape\n",
    "print(\"read {0} documents with {1} vocabulary\".format(dc,vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display our texts\n",
    "novel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=10,               \n",
    "                                      max_iter=5,\n",
    "                                      learning_method='batch',\n",
    "                                      random_state=None)    \n",
    "# get fitted data and transformed matrix\n",
    "lda_data = lda_model.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the features to a simple list\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# how many words do we want to extract for each topic?\n",
    "n_words = 100\n",
    "\n",
    "# now produce topics\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "        print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now re-run the above by building a custom stoplist to remove \n",
    "# some frequenly used words that you see in the above list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the words from the first topic\n",
    "data = lda_model.components_[0]\n",
    "word_index = data.argsort()[:-20 - 1:-1]\n",
    "\n",
    "x = [feature_names[x] for x in word_index]\n",
    "y = [data[x] for x in word_index]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Topic #0: Important Tokens\")\n",
    "\n",
    "# what should these be?\n",
    "# add labels...\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Document â€” Topic Matrix\n",
    "import pandas as pd\n",
    "\n",
    "lda_transformed_data = lda_model.fit_transform(counts)\n",
    "\n",
    "# make lists of topics and texts for labels\n",
    "topics = [\"Topic \" + str(i) for i in range(lda_model.n_components)]\n",
    "texts = [\"Text \" + str(i) for i in range(len(novel_data))]\n",
    "\n",
    "# put data into a special datatype called a Pandas DataFrame\n",
    "topic_chart = pd.DataFrame(np.round(lda_transformed_data, 3), columns=topics, index=texts)\n",
    "\n",
    "# extract the most dominant topic by searching for largest value\n",
    "topic_chart['Dominant Topic'] = np.argmax(df_document_topic.values, axis=1)\n",
    "\n",
    "# Display\n",
    "topic_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, try to put together what we've learned from the above to retrieve \n",
    "# all the topics for one of your texts.\n",
    "\n",
    "# topics are stored in lda_transformed_data. We can use np.round to get \n",
    "# more legible values (we most likely want three decimal values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to TF-IDF frequencies \n",
    "from sklearn.feature_extraction import text\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)\n",
    "\n",
    "# fit to NMF model\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# n_components is the number of topics (same as LDA)\n",
    "nmf_model = NMF(n_components=15,\n",
    "                init='random', \n",
    "                random_state=0)\n",
    "\n",
    "# fit \n",
    "nmf_output = nmf_model.fit_transform(tfidf)\n",
    "nmf_data = nmf_model.fit(tfidf)\n",
    "\n",
    "# reload feature names\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# max number of words per topic to display\n",
    "n_words = 100\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "        print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Document â€” Topic Matrix\n",
    "import pandas as pd\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic \" + str(i) for i in range(nmf_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Text \" + str(i) for i in range(len(novel_data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(nmf_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['Dominant Topic'] = dominant_topic\n",
    "df_document_topic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
